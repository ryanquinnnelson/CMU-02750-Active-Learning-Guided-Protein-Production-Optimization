{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "constitutional-substitute",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "second-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from modAL.models import ActiveLearner, CommitteeRegressor\n",
    "from modAL.disagreement import max_std_sampling\n",
    "\n",
    "### Suppresses Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-siemens",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "- Try Gaussian process as regressor. There may be a performance difference using it in Active Learning vs Bayesian Optimization.\n",
    "- Types of regression to try\n",
    "    - linear\n",
    "    - ridge\n",
    "    - lasso\n",
    "    - polynomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-breakfast",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aquatic-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_combinations(ranges_dict, feature):\n",
    "    \n",
    "    # min of all other features, max of this feature\n",
    "    min_combo = []\n",
    "    for key in ranges_dict.keys():\n",
    "        \n",
    "        if key == feature:\n",
    "            min_combo.append(min(ranges_dict[key]))\n",
    "        else:\n",
    "            min_combo.append(max(ranges_dict[key]))\n",
    "        \n",
    "\n",
    "    # max of all other features, min of this feature\n",
    "    max_combo = []\n",
    "    for key in ranges_dict.keys():\n",
    "        \n",
    "        if key == feature:\n",
    "            max_combo.append(max(ranges_dict[key]))\n",
    "        else:\n",
    "            max_combo.append(min(ranges_dict[key]))\n",
    "    \n",
    "    return tuple(min_combo), tuple(max_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "infinite-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(df, combo):\n",
    "    \n",
    "    # split into components\n",
    "    Mg_glutamate, K_glutamate, Amino_Acid, tRNA, coA, NAD, cAMP, Folinic_Acid, Spermidine, Three_PGA, NTP = combo\n",
    "    \n",
    "    # get index of matching row in data\n",
    "    a = df.index[\n",
    "              np.isclose(df['Mg-glutamate (mM)'], Mg_glutamate) \\\n",
    "            & np.isclose(df['K-glutamate (mM)'], K_glutamate) \\\n",
    "            & np.isclose(df['Amino Acid (mM)'], Amino_Acid) \\\n",
    "            & np.isclose(df['tRNA (mg/ml)'], tRNA) \\\n",
    "            & np.isclose(df['coA (mM)'],coA)  \\\n",
    "            & np.isclose(df['NAD (mM)'], NAD) \\\n",
    "            & np.isclose(df['cAMP (mM)'], cAMP) \\\n",
    "            & np.isclose(df['Folinic Acid (mM)'], Folinic_Acid) \\\n",
    "            & np.isclose(df['Spermidine (mM)'], Spermidine) \\\n",
    "            & np.isclose(df['3-PGA (mM)'], Three_PGA) \\\n",
    "            & np.isclose(df['NTP (mM)'], NTP)]\n",
    "    return a.tolist()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "brief-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_22_idx(df, ranges_dict):\n",
    "    \n",
    "    initial_idx = []\n",
    "    \n",
    "    # for each feature\n",
    "    for key in ranges_dict.keys():\n",
    "        min_combo, max_combo = get_min_max_combinations(ranges_dict,key)\n",
    "        idx = get_index(df, min_combo)\n",
    "        initial_idx.append(idx)\n",
    "        idx = get_index(df, max_combo)\n",
    "        initial_idx.append(idx)\n",
    "        \n",
    "    return np.array(initial_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-supervision",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-outside",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "amateur-shift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1017, 12)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/DataPool.csv')\n",
    "print(data.shape)\n",
    "data.head(5)\n",
    "cols = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-oliver",
   "metadata": {},
   "source": [
    "### Split into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "disciplinary-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1017, 11)\n",
      "(1017,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(['Yield'],axis=1)\n",
    "print(X.shape)\n",
    "y_pool = data['Yield'].to_numpy()\n",
    "print(y_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-asthma",
   "metadata": {},
   "source": [
    "### Scale data\n",
    "So all variables have the same mean and std dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "comprehensive-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_pool = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-woman",
   "metadata": {},
   "source": [
    "# Define initial dataset\n",
    "- 11 data points in which each compound is minimized except one is maximized\n",
    "- 11 data pionts in which each compound is maximized except one is minimized\n",
    "- 80 random data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "welcome-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_dict = {\n",
    "    'Mg-glutamate (mM)':[0.4,1.2,2,4],\n",
    "    'K-glutamate (mM)':[8,24,40,80],\n",
    "    'Amino Acid (mM)': [0.15,0.45,0.75,1.5],\n",
    "    'tRNA (mg/ml)':[0.02,0.06,0.1,0.2],\n",
    "    'coA (mM)':[0.026,0.078,0.13,0.26],\n",
    "    'NAD (mM)':[0.033,0.099,0.165,0.33],\n",
    "    'cAMP (mM)':[0.075,0.225,0.375,0.75],\n",
    "    'Folinic Acid (mM)':[0.0068,0.0204,0.034,0.068],\n",
    "    'Spermidine (mM)':[0.1,0.3,0.5,1],\n",
    "    '3-PGA (mM)':[3,9,15,30],\n",
    "    'NTP (mM)':[0.15,0.45,0.75,1.5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "private-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 11)\n",
      "(22,)\n",
      "(995, 11)\n",
      "(995,)\n",
      "(80, 11)\n",
      "(80,)\n",
      "(915, 11)\n",
      "(915,)\n",
      "(102, 11)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "idx_first_22 = get_initial_22_idx(data, ranges_dict)\n",
    "\n",
    "X_training_22 = X_pool[idx_first_22]\n",
    "y_training_22 = y_pool[idx_first_22]\n",
    "print(X_training_22.shape)\n",
    "print(y_training_22.shape)\n",
    "\n",
    "# remove from pool\n",
    "X_pool = np.delete(X_pool, idx_first_22, axis=0)\n",
    "y_pool = np.delete(y_pool, idx_first_22)   \n",
    "print(X_pool.shape)\n",
    "print(y_pool.shape)\n",
    "\n",
    "# chose 80 points from pool at random\n",
    "idx_random_80 = np.random.choice(range(len(X_pool)), size=80, replace=False)\n",
    "X_training_80, y_training_80 = X_pool[idx_random_80], y_pool[idx_random_80]\n",
    "print(X_training_80.shape)\n",
    "print(y_training_80.shape)\n",
    "\n",
    "# remove 80 points from pool\n",
    "X_pool = np.delete(X_pool, idx_random_80, axis=0)\n",
    "y_pool = np.delete(y_pool, idx_random_80)   \n",
    "print(X_pool.shape)\n",
    "print(y_pool.shape) \n",
    "\n",
    "# combine training sets together\n",
    "X_training = np.append(X_training_22, X_training_80, axis=0)\n",
    "y_training = np.append(y_training_22, y_training_80, axis=0)\n",
    "print(X_training.shape)\n",
    "print(y_training.shape)\n",
    "\n",
    "assert X_training.shape == (102,11)\n",
    "assert y_training.shape == (102, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-motivation",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "essential-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RfWrapper(RandomForestRegressor):  # superclass\n",
    "    \"\"\"\n",
    "    Wrapper class for RandomForestRegressor which modifies predict() method to include second argument return_std.\n",
    "    This argument is expected by\n",
    "    modAL library for active learning regression. Provided by course instructors.\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "        if return_std:\n",
    "            ys = np.array([e.predict(X) for e in self.estimators_])\n",
    "            return np.mean(ys, axis=0).ravel(), np.std(ys, axis=0).ravel()\n",
    "        return super().predict(X).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "subsequent-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrWrapper(LinearRegression):  #superclass\n",
    "    \"\"\"\n",
    "    Wrapper class for RandomForestRegressor which modifies predict() method to include second argument return_std.\n",
    "    This argument is expected by\n",
    "    modAL library for active learning regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, X, return_std=False):\n",
    "        if return_std:\n",
    "            ys = np.array([e.predict(X) for e in self.estimators_])\n",
    "            return np.mean(ys, axis=0).ravel(), np.std(ys, axis=0).ravel()\n",
    "        return super().predict(X).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "focal-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sample(optimizer, X_pool, y_pool):\n",
    "\n",
    "    # call the query strategy defined in the learner to obtain a new sample\n",
    "    query_idx, query_sample = optimizer.query(X_pool)\n",
    "\n",
    "    # modify indexing to interpret as collection of one element with d features\n",
    "    query_sample_reshaped = query_sample.reshape(1, -1)\n",
    "\n",
    "    # obtain the query label\n",
    "    query_label = y_pool[query_idx]\n",
    "\n",
    "    # modify indexing to interpret as 1D array of one element\n",
    "    query_label_reshaped = query_label.reshape(1, )\n",
    "\n",
    "    return query_sample_reshaped, query_label_reshaped, query_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adolescent-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(learner, X_pool, y_pool, batch_size):\n",
    "    \n",
    "    n_col = X_pool.shape[1]\n",
    "    X_batch = np.zeros((batch_size, n_col))\n",
    "    y_batch = np.zeros((batch_size,))\n",
    "\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        X_sample, y_sample, query_idx = get_next_sample(learner, X_pool, y_pool)\n",
    "        \n",
    "        # add to batch\n",
    "        X_batch[i] = X_sample\n",
    "        y_batch[i] = y_sample\n",
    "        \n",
    "        # remove queried point from pool\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)     \n",
    "        \n",
    "    return X_batch, y_batch, X_pool, y_pool\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "based-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_regression_model(optimizer, X_test, y_test):\n",
    "\n",
    "    y_pred = optimizer.predict(X_test, return_std=False)\n",
    "    r2 = r2_score(y_test, y_pred)  # y_true, y_pred\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "filled-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_committee(n_learner, X_training, y_training, seed):\n",
    "    \n",
    "    # get bootstrapped initial training set for each learner\n",
    "    initial_idx = []\n",
    "    for i in range(n_learner):\n",
    "        initial_idx.append(np.random.choice(len(X_training), size=len(X_training), replace=True))\n",
    "        \n",
    "       \n",
    "    # initialize learners for Committee\n",
    "    learner_list = []\n",
    "    for idx in initial_idx:\n",
    "        al = ActiveLearner(\n",
    "            estimator=RfWrapper(n_estimators=20, max_depth=6, random_state=seed),\n",
    "            X_training=X_training[idx],\n",
    "            y_training=y_training[idx])\n",
    "        learner_list.append(al)\n",
    "        \n",
    "    # initializing the Committee\n",
    "    committee = CommitteeRegressor(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=max_std_sampling\n",
    "    )\n",
    "\n",
    "    return committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "threaded-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_regression(learner, X_pool, y_pool, n_batch, batch_size, initial):\n",
    "    print('X_pool size, y_pool size', X_pool.shape, y_pool.shape)\n",
    "#     selected = [initial]\n",
    "#     data_sets = [(X_pool,y_pool)]\n",
    "    for b in range(n_batch):\n",
    "        \n",
    "        # get next set of points to learn\n",
    "        X_batch, y_batch, X_pool, y_pool = get_next_batch(learner, X_pool, y_pool, batch_size)\n",
    "        print('X_pool size, y_pool size', X_pool.shape, y_pool.shape)\n",
    "        \n",
    "#         # save data\n",
    "#         batch_col = np.array([b]*len(X_batch)).reshape(-1,1)\n",
    "#         y_col = y_batch.reshape(-1,1)\n",
    "#         result = np.append(X_batch,y_col, axis=1)\n",
    "#         selected.append(result)\n",
    "        \n",
    "#         # save dataset\n",
    "#         data_sets.append((X_pool,y_pool))\n",
    "    \n",
    "        # use new sample to update the model\n",
    "        learner.teach(X_batch, y_batch)\n",
    "    \n",
    "#     return selected,data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extensive-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915, 11)\n",
      "(915,)\n",
      "X_pool size, y_pool size (915, 11) (915,)\n",
      "X_pool size, y_pool size (914, 11) (914,)\n",
      "X_pool size, y_pool size (913, 11) (913,)\n",
      "X_pool size, y_pool size (912, 11) (912,)\n",
      "X_pool size, y_pool size (911, 11) (911,)\n",
      "X_pool size, y_pool size (910, 11) (910,)\n",
      "X_pool size, y_pool size (909, 11) (909,)\n",
      "X_pool size, y_pool size (908, 11) (908,)\n",
      "X_pool size, y_pool size (907, 11) (907,)\n",
      "X_pool size, y_pool size (906, 11) (906,)\n",
      "X_pool size, y_pool size (905, 11) (905,)\n",
      "CPU times: user 6.42 s, sys: 89.9 ms, total: 6.51 s\n",
      "Wall time: 6.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# copy data for this section\n",
    "X_cp = copy.deepcopy(X_pool)\n",
    "y_cp = copy.deepcopy(y_pool)\n",
    "print(X_cp.shape)\n",
    "print(y_cp.shape)\n",
    "\n",
    "\n",
    "\n",
    "# define committee\n",
    "committee = build_committee(25, X_training, y_training, seed)\n",
    "\n",
    "# save initial training data as first batch\n",
    "initial = np.append(X_training, y_training.reshape(-1,1),axis=1)\n",
    "\n",
    "# active regression\n",
    "n_batch = 10\n",
    "batch_size = 1\n",
    "run_active_regression(committee, X_cp, y_cp, n_batch, batch_size, initial)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-comment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-compatibility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
